# Доказательство того, что модель линейна. А значит может быть представлена в виде нейронной сети с входным слоем и выходным. Но так делать совсем не обязательно!
[Тестовый граф](Test_graph.png)

На рисунке видно, что графе есть цикл. Его надо видоизменить на тот, что ниже:
[Тестовый граф без цикла](Test_graph_no_cycle.png)

Доказательство.
Вес между вершинами (нодами) обозначается $\omega$ + цифры снизу. Пример: вес между $x_1$ и $x_2$ - $\omega_{12}$.
Даны значения всех вершин (исторические значения), кроме $x_2^1$ и $x_2^2$ (1 - один штрих, 2 - два штриха соответственно), но $x_2 = x_2^1 + x_2^2$ - известно. $x_1$ - прогнозируемая величина, которая так же дана.
Тогда:
1) $x_{2}^1 = x_1 * \omega_{12}$
2) $x_{2}^2 = x_8 * \omega_{82}^2 = x_5 * \omega_{58}  * \omega_{82}^2 = x_{2}^1 * \omega_{25}^1 * \omega_{58}  * \omega_{82}^2  = x_1 * \omega_{12} *  \omega_{25}^1 * \omega_{58}  * \omega_{82}^2$
3) $x_2 = x_{2}^1 + x_{2}^2 = x_1 * \omega_{12} + x_1 * \omega_{12} *  \omega_{25}^1 * \omega_{58}  * \omega_{82}^2 = x_1 * \omega_{12} * (1 + \omega_{25}^1 * \omega_{58}  * \omega_{82}^2) = x_1 * \beta$
4) $\beta = \omega_{12} * (1 + \omega_{25}^1 * \omega_{58}  * \omega_{82}^2)$
5) $x_3 = x_1 * \omega_{13}$
6) $x_4 = x_2^1 * \omega_{24}^1 = x_1 * \omega_{12} * \omega_{24}^1$
7) $x_5 = x_2^1 * \omega_{25}^1 = x_1 * \omega_{12} * \omega_{25}^1$
8) $x_6 = x_4 * \omega_{46} = x_1 * \omega_{12} * \omega_{24}^1 * \omega_{46}$
9) $x_7 = x_4 * \omega_{47} = x_1 * \omega_{12} * \omega_{24}^1 * \omega_{47}$
10) $x_8 = x_5 * \omega_{58} = x_2^1 * \omega_{25}^1 * \omega_{58} = x_1 * \omega_{12} * \omega_{25}^1 * \omega_{58}$

Могу дальше выписывать выражения и я это сделаю, но уже видно что задачу можно решать и без PyTorch. Зависимости линейные.
